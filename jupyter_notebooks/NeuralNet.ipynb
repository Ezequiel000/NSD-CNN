{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k64Z8hXNy-sh"
   },
   "source": [
    "#Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_iftfZn2Ndo",
    "outputId": "c605bfc6-d0e7-4b27-9949-51e879c81991"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "FMRI_DIR_STND = 'C:/Users/007303173/Documents/nsd_data/standardized-betas/subj01'\n",
    "PATH = 'C:/Users/007303173/Documents/nsd_data/saved_models/subj01' #path for checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gurPuhWfJLPG"
   },
   "source": [
    "#Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "91i9gOEC6bow"
   },
   "outputs": [],
   "source": [
    "class DatasetNSD(Dataset):\n",
    "  def __init__(self):\n",
    "      #fmri_files will store all the betas files in the given directory\n",
    "      self.dir = FMRI_DIR_STND\n",
    "      self.fmri_files = self.getDirFiles()\n",
    "      # num_of_scans is set by getRanges\n",
    "      self.num_of_scans = 0\n",
    "      # index_ranges is a dictionery with index range as key and corresponding file as value\n",
    "      self.index_ranges =  self.getRanges()\n",
    "      self.file_handlers = {}\n",
    "      self.open_files()\n",
    "\n",
    "  def __del__(self):\n",
    "    for key, value in self.file_handlers.items():\n",
    "      value.close()\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_of_scans \n",
    "\n",
    "#Input: hdf5 file name \n",
    "#Output: store hdf5 in deque, return file object\n",
    "  def open_files(self): \n",
    "    for file_name in self.fmri_files:\n",
    "      path = os.path.join(FMRI_DIR_STND, file_name)\n",
    "      self.file_handlers[file_name] = h5py.File(path, 'r')\n",
    "\n",
    "   \n",
    "  def __getitem__(self, index):\n",
    "    #index will be 0-17908\n",
    "    # files come in diffirent sizes of fmri scans.\n",
    "    # a file will be chose based on the range of the index \n",
    "    for key in self.index_ranges:\n",
    "      index_range = list(key)\n",
    "      if (index in index_range):\n",
    "        f = self.file_handlers[self.index_ranges[key]]\n",
    "        indx = index - f['startIndx/i'][0]\n",
    "        sample = torch.FloatTensor(np.array(f['betas/b'][indx]))\n",
    "        label = torch.FloatTensor(np.array(f['labels/l'][indx]))\n",
    "        break\n",
    "    return (sample, label)\n",
    "\n",
    "     \n",
    "  def getDirFiles(self):\n",
    "     files = [f for f in os.listdir(self.dir) if \n",
    "              os.path.isfile(os.path.join(self.dir, f)) and\n",
    "              f[-5:] == '.hdf5']\n",
    "\n",
    "     files.sort()                          \n",
    "     return files\n",
    "\n",
    "  def getRanges(self):\n",
    "    ranges = {}\n",
    "    previous = 0\n",
    "    count = 0\n",
    "    for x in self.fmri_files:\n",
    "      with h5py.File(os.path.join(self.dir, x), \"r\") as f:    \n",
    "          size = f['labels/l'].shape[0]\n",
    "          count = count + size\n",
    "          if previous == 0:\n",
    "            ranges[range(size)] = x\n",
    "            previous = size\n",
    "          else:\n",
    "            size = size + previous\n",
    "            i = range(previous,size)\n",
    "            ranges[i] = x\n",
    "            previous = size\n",
    "    self.num_of_scans = count\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bTp53LFIqZf"
   },
   "source": [
    "#NeuralNet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J8lB0a33Is9G"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.kernel = 2\n",
    "        self.stride = 2 \n",
    "        self.conv_stack = nn.Sequential(\n",
    "        nn.Conv3d(1,out_channels=64 , kernel_size = 3, stride = 3),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv3d(in_channels=64,out_channels=32 , kernel_size = self.kernel, stride = self.stride),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv3d(in_channels=32,out_channels=16 , kernel_size = self.kernel, stride = self.stride),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Flatten()\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "        nn.Linear(4608,2000),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(2000,1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(1000,500),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(500,256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "          )\n",
    "\n",
    "    # # [(input_volume−kernel_size)/stride]+1\n",
    "    # def output_size(self,volume):\n",
    "    #   size = ((volume -self.kernel)//self.stride) + 1\n",
    "    #   return size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x.unsqueeze(dim=1))\n",
    "        x = self.linear_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYzRjZlLny_5"
   },
   "source": [
    "#DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNnQPJOdlXtA",
    "outputId": "f6289deb-fd5b-482c-aa2b-27085f11cc33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14921\n",
      "2984\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 199\n",
    "ds = DatasetNSD()\n",
    "split = len(ds)//6\n",
    "split = len(ds)-split\n",
    "#split =14925\n",
    "train_dataset = Subset(ds, range(len(ds))[: split] )\n",
    "print(len(train_dataset))\n",
    "test_dataset = Subset(ds, range(len(ds))[split: ])\n",
    "print(len(test_dataset))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q18L9Sj4I9Lf"
   },
   "source": [
    "#Initialize NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtlyaMcFnsgI",
    "outputId": "28b00159-535b-4c6b-98ee-7b600209a4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "CNN(\n",
      "  (conv_stack): Sequential(\n",
      "    (0): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(3, 3, 3))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=4608, out_features=2000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=1000, out_features=500, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=500, out_features=256, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (12): ReLU()\n",
      "    (13): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (14): ReLU()\n",
      "    (15): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#device config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#hyper parameter\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "modelNN = CNN().to(device)\n",
    "# modelNN.load_state_dict(torch.load(os.path.join(PATH,'model_weights.pth')))\n",
    "# weight = torch.FloatTensor([.322]).to(device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "criterion = torch.optim.Adam(modelNN.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "print(modelNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGLKWaKvbebA"
   },
   "source": [
    "#Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KqiBL1e2SwL"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def trainingLoop(train_ld, model, loss_function, optimizer , test_ld):\n",
    "  scaler = GradScaler()\n",
    "  trues = []\n",
    "  preds = []\n",
    "  for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    n_total_steps = len(train_ld)\n",
    "    for i, (sample, true_label) in enumerate(train_ld):\n",
    "      sample = sample.to(device)\n",
    "      trues.extend(true_label.numpy())\n",
    "      true_label = true_label.to(device).unsqueeze(dim=1)\n",
    "      pred_label = model(sample)\n",
    "      loss = loss_function(pred_label, true_label)\n",
    "      train_loss =  loss.item()\n",
    "      avg_loss += train_loss\n",
    "      #backward pass\n",
    "      optimizer.zero_grad()\n",
    "      scaler.scale(loss).backward()\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      #predictioins\n",
    "      pred = torch.sigmoid(pred_label).detach().cpu()\n",
    "      preds.extend(np.round(pred.reshape(-1)).numpy())\n",
    "      if (i+1) % 5 == 0:\n",
    "        print(f'epoch {epoch+1}/{EPOCHS},step {i+1}/{n_total_steps}, loss = {(train_loss):.4f} ')   \n",
    "    score = f1_score(trues, preds)\n",
    "    print(f'F1 score for training set is: {score}')\n",
    "    torch.save(model.state_dict(), os.path.join(PATH,'model_weights.pth'))\n",
    "    print(f'Average train loss for epoch: {avg_loss/n_total_steps} ')\n",
    "    predTesting(test_ld, model)\n",
    "\n",
    "def predTesting(loader, model ): \n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    trues = []\n",
    "    preds = []\n",
    "    for  sample, true_label in tqdm(loader):\n",
    "      sample = sample.to(device)\n",
    "      trues.extend(true_label.numpy())\n",
    "      true_label = true_label.to(device).unsqueeze(dim=1)\n",
    "      pred_label = model(sample)\n",
    "      pred = torch.sigmoid(pred_label).cpu()\n",
    "      preds.extend(np.round(pred.reshape(-1)).numpy())\n",
    "    score = f1_score(trues, preds)\n",
    "    print(f'F1 score for validation set is: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eN9LVlv0Hxkc",
    "outputId": "a1a7b7a3-85e0-428c-8143-a538e8eac2e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:35<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for training set is: 0.8692686623721106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predTesting(test_loader, modelNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8synMMwQ4t2",
    "outputId": "bc056f93-c9b9-4b4a-fd51-75edd67d02f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20,step 5/75, loss = 0.7148 \n",
      "epoch 1/20,step 10/75, loss = 0.7046 \n",
      "epoch 1/20,step 15/75, loss = 0.6740 \n",
      "epoch 1/20,step 20/75, loss = 0.5975 \n",
      "epoch 1/20,step 25/75, loss = 0.6832 \n",
      "epoch 1/20,step 30/75, loss = 0.5003 \n",
      "epoch 1/20,step 35/75, loss = 0.5886 \n",
      "epoch 1/20,step 40/75, loss = 0.5432 \n",
      "epoch 1/20,step 45/75, loss = 0.5350 \n",
      "epoch 1/20,step 50/75, loss = 0.6557 \n",
      "epoch 1/20,step 55/75, loss = 0.5083 \n",
      "epoch 1/20,step 60/75, loss = 0.6119 \n",
      "epoch 1/20,step 65/75, loss = 0.5412 \n",
      "epoch 1/20,step 70/75, loss = 0.5410 \n",
      "epoch 1/20,step 75/75, loss = 0.5551 \n",
      "F1 score for training set is: 0.7917440486240082\n",
      "Average train loss for epoch: 0.5972925515969595 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:05<00:00, 12.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8692686623721106\n",
      "epoch 2/20,step 5/75, loss = 0.5635 \n",
      "epoch 2/20,step 10/75, loss = 0.5773 \n",
      "epoch 2/20,step 15/75, loss = 0.6005 \n",
      "epoch 2/20,step 20/75, loss = 0.5822 \n",
      "epoch 2/20,step 25/75, loss = 0.5704 \n",
      "epoch 2/20,step 30/75, loss = 0.6153 \n",
      "epoch 2/20,step 35/75, loss = 0.5635 \n",
      "epoch 2/20,step 40/75, loss = 0.6061 \n",
      "epoch 2/20,step 45/75, loss = 0.5858 \n",
      "epoch 2/20,step 50/75, loss = 0.4883 \n",
      "epoch 2/20,step 55/75, loss = 0.4799 \n",
      "epoch 2/20,step 60/75, loss = 0.5433 \n",
      "epoch 2/20,step 65/75, loss = 0.5712 \n",
      "epoch 2/20,step 70/75, loss = 0.5827 \n",
      "epoch 2/20,step 75/75, loss = 0.5278 \n",
      "F1 score for training set is: 0.8273801167244941\n",
      "Average train loss for epoch: 0.5543103003501892 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:05<00:00, 12.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8692686623721106\n",
      "epoch 3/20,step 5/75, loss = 0.6292 \n",
      "epoch 3/20,step 10/75, loss = 0.5163 \n",
      "epoch 3/20,step 15/75, loss = 0.5128 \n",
      "epoch 3/20,step 20/75, loss = 0.6133 \n",
      "epoch 3/20,step 25/75, loss = 0.5579 \n",
      "epoch 3/20,step 30/75, loss = 0.4788 \n",
      "epoch 3/20,step 35/75, loss = 0.5385 \n",
      "epoch 3/20,step 40/75, loss = 0.5537 \n",
      "epoch 3/20,step 45/75, loss = 0.5571 \n",
      "epoch 3/20,step 50/75, loss = 0.5979 \n",
      "epoch 3/20,step 55/75, loss = 0.5615 \n",
      "epoch 3/20,step 60/75, loss = 0.4850 \n",
      "epoch 3/20,step 65/75, loss = 0.5517 \n",
      "epoch 3/20,step 70/75, loss = 0.5433 \n",
      "epoch 3/20,step 75/75, loss = 0.5660 \n",
      "F1 score for training set is: 0.8384848086281731\n",
      "Average train loss for epoch: 0.5415147646268209 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:05<00:00, 12.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8692686623721106\n",
      "epoch 4/20,step 5/75, loss = 0.5337 \n",
      "epoch 4/20,step 10/75, loss = 0.4589 \n",
      "epoch 4/20,step 15/75, loss = 0.4725 \n",
      "epoch 4/20,step 20/75, loss = 0.5154 \n",
      "epoch 4/20,step 25/75, loss = 0.5049 \n",
      "epoch 4/20,step 30/75, loss = 0.4870 \n",
      "epoch 4/20,step 35/75, loss = 0.4885 \n",
      "epoch 4/20,step 40/75, loss = 0.4578 \n",
      "epoch 4/20,step 45/75, loss = 0.5099 \n",
      "epoch 4/20,step 50/75, loss = 0.4718 \n",
      "epoch 4/20,step 55/75, loss = 0.4876 \n",
      "epoch 4/20,step 60/75, loss = 0.5059 \n",
      "epoch 4/20,step 65/75, loss = 0.4785 \n",
      "epoch 4/20,step 70/75, loss = 0.4324 \n",
      "epoch 4/20,step 75/75, loss = 0.4086 \n",
      "F1 score for training set is: 0.8452295428824484\n",
      "Average train loss for epoch: 0.4795743787288666 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8764634638675818\n",
      "epoch 5/20,step 5/75, loss = 0.4531 \n",
      "epoch 5/20,step 10/75, loss = 0.4093 \n",
      "epoch 5/20,step 15/75, loss = 0.4436 \n",
      "epoch 5/20,step 20/75, loss = 0.4045 \n",
      "epoch 5/20,step 25/75, loss = 0.4621 \n",
      "epoch 5/20,step 30/75, loss = 0.4144 \n",
      "epoch 5/20,step 35/75, loss = 0.4046 \n",
      "epoch 5/20,step 40/75, loss = 0.4452 \n",
      "epoch 5/20,step 45/75, loss = 0.4155 \n",
      "epoch 5/20,step 50/75, loss = 0.4430 \n",
      "epoch 5/20,step 55/75, loss = 0.3063 \n",
      "epoch 5/20,step 60/75, loss = 0.3792 \n",
      "epoch 5/20,step 65/75, loss = 0.3631 \n",
      "epoch 5/20,step 70/75, loss = 0.3806 \n",
      "epoch 5/20,step 75/75, loss = 0.4412 \n",
      "F1 score for training set is: 0.852229777781329\n",
      "Average train loss for epoch: 0.4052902948856354 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:05<00:00, 12.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.891381100726895\n",
      "epoch 6/20,step 5/75, loss = 0.3738 \n",
      "epoch 6/20,step 10/75, loss = 0.3593 \n",
      "epoch 6/20,step 15/75, loss = 0.3681 \n",
      "epoch 6/20,step 20/75, loss = 0.3963 \n",
      "epoch 6/20,step 25/75, loss = 0.3928 \n",
      "epoch 6/20,step 30/75, loss = 0.3096 \n",
      "epoch 6/20,step 35/75, loss = 0.3891 \n",
      "epoch 6/20,step 40/75, loss = 0.3884 \n",
      "epoch 6/20,step 45/75, loss = 0.4070 \n",
      "epoch 6/20,step 50/75, loss = 0.4686 \n",
      "epoch 6/20,step 55/75, loss = 0.3827 \n",
      "epoch 6/20,step 60/75, loss = 0.3396 \n",
      "epoch 6/20,step 65/75, loss = 0.2624 \n",
      "epoch 6/20,step 70/75, loss = 0.3002 \n",
      "epoch 6/20,step 75/75, loss = 0.3883 \n",
      "F1 score for training set is: 0.8596604601693659\n",
      "Average train loss for epoch: 0.3562443419297536 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:05<00:00, 12.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.884486979746272\n",
      "epoch 7/20,step 5/75, loss = 0.2961 \n",
      "epoch 7/20,step 10/75, loss = 0.3233 \n",
      "epoch 7/20,step 15/75, loss = 0.2660 \n",
      "epoch 7/20,step 20/75, loss = 0.2924 \n",
      "epoch 7/20,step 25/75, loss = 0.3416 \n",
      "epoch 7/20,step 30/75, loss = 0.3526 \n",
      "epoch 7/20,step 35/75, loss = 0.3692 \n",
      "epoch 7/20,step 40/75, loss = 0.3018 \n",
      "epoch 7/20,step 45/75, loss = 0.2669 \n",
      "epoch 7/20,step 50/75, loss = 0.3380 \n",
      "epoch 7/20,step 55/75, loss = 0.3127 \n",
      "epoch 7/20,step 60/75, loss = 0.2947 \n",
      "epoch 7/20,step 65/75, loss = 0.2713 \n",
      "epoch 7/20,step 70/75, loss = 0.2703 \n",
      "epoch 7/20,step 75/75, loss = 0.2695 \n",
      "F1 score for training set is: 0.8674582331258335\n",
      "Average train loss for epoch: 0.30060862243175507 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:05<00:00, 12.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.90198563582594\n",
      "epoch 8/20,step 5/75, loss = 0.3073 \n",
      "epoch 8/20,step 10/75, loss = 0.3167 \n",
      "epoch 8/20,step 15/75, loss = 0.2746 \n",
      "epoch 8/20,step 20/75, loss = 0.3196 \n",
      "epoch 8/20,step 25/75, loss = 0.2212 \n",
      "epoch 8/20,step 30/75, loss = 0.3368 \n",
      "epoch 8/20,step 35/75, loss = 0.1872 \n",
      "epoch 8/20,step 40/75, loss = 0.2257 \n",
      "epoch 8/20,step 45/75, loss = 0.2190 \n",
      "epoch 8/20,step 50/75, loss = 0.2603 \n",
      "epoch 8/20,step 55/75, loss = 0.2487 \n",
      "epoch 8/20,step 60/75, loss = 0.2562 \n",
      "epoch 8/20,step 65/75, loss = 0.2764 \n",
      "epoch 8/20,step 70/75, loss = 0.2493 \n",
      "epoch 8/20,step 75/75, loss = 0.2654 \n",
      "F1 score for training set is: 0.8748529963075754\n",
      "Average train loss for epoch: 0.26264482498168945 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:05<00:00, 12.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.899913718723037\n",
      "epoch 9/20,step 5/75, loss = 0.2423 \n",
      "epoch 9/20,step 10/75, loss = 0.1751 \n",
      "epoch 9/20,step 15/75, loss = 0.1431 \n",
      "epoch 9/20,step 20/75, loss = 0.2149 \n",
      "epoch 9/20,step 25/75, loss = 0.2220 \n",
      "epoch 9/20,step 30/75, loss = 0.2749 \n",
      "epoch 9/20,step 35/75, loss = 0.3571 \n",
      "epoch 9/20,step 40/75, loss = 0.2760 \n",
      "epoch 9/20,step 45/75, loss = 0.2413 \n",
      "epoch 9/20,step 50/75, loss = 0.2440 \n",
      "epoch 9/20,step 55/75, loss = 0.1999 \n",
      "epoch 9/20,step 60/75, loss = 0.2122 \n",
      "epoch 9/20,step 65/75, loss = 0.2124 \n",
      "epoch 9/20,step 70/75, loss = 0.2488 \n",
      "epoch 9/20,step 75/75, loss = 0.2352 \n",
      "F1 score for training set is: 0.8818873038844095\n",
      "Average train loss for epoch: 0.2271414491534233 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.9044585987261147\n",
      "epoch 10/20,step 5/75, loss = 0.2234 \n",
      "epoch 10/20,step 10/75, loss = 0.1673 \n",
      "epoch 10/20,step 15/75, loss = 0.2327 \n",
      "epoch 10/20,step 20/75, loss = 0.2400 \n",
      "epoch 10/20,step 25/75, loss = 0.2070 \n",
      "epoch 10/20,step 30/75, loss = 0.1888 \n",
      "epoch 10/20,step 35/75, loss = 0.2221 \n",
      "epoch 10/20,step 40/75, loss = 0.1357 \n",
      "epoch 10/20,step 45/75, loss = 0.1367 \n",
      "epoch 10/20,step 50/75, loss = 0.1836 \n",
      "epoch 10/20,step 55/75, loss = 0.2074 \n",
      "epoch 10/20,step 60/75, loss = 0.2399 \n",
      "epoch 10/20,step 65/75, loss = 0.2416 \n",
      "epoch 10/20,step 70/75, loss = 0.1855 \n",
      "epoch 10/20,step 75/75, loss = 0.1578 \n",
      "F1 score for training set is: 0.8888574742943149\n",
      "Average train loss for epoch: 0.1814665914575259 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8914694059848147\n",
      "epoch 11/20,step 5/75, loss = 0.1353 \n",
      "epoch 11/20,step 10/75, loss = 0.1444 \n",
      "epoch 11/20,step 15/75, loss = 0.1443 \n",
      "epoch 11/20,step 20/75, loss = 0.1135 \n",
      "epoch 11/20,step 25/75, loss = 0.1987 \n",
      "epoch 11/20,step 30/75, loss = 0.1326 \n",
      "epoch 11/20,step 35/75, loss = 0.1521 \n",
      "epoch 11/20,step 40/75, loss = 0.1876 \n",
      "epoch 11/20,step 45/75, loss = 0.1982 \n",
      "epoch 11/20,step 50/75, loss = 0.1363 \n",
      "epoch 11/20,step 55/75, loss = 0.1312 \n",
      "epoch 11/20,step 60/75, loss = 0.1377 \n",
      "epoch 11/20,step 65/75, loss = 0.0874 \n",
      "epoch 11/20,step 70/75, loss = 0.1276 \n",
      "epoch 11/20,step 75/75, loss = 0.1189 \n",
      "F1 score for training set is: 0.8953079836066196\n",
      "Average train loss for epoch: 0.1538668939471245 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8940235503221506\n",
      "epoch 12/20,step 5/75, loss = 0.0743 \n",
      "epoch 12/20,step 10/75, loss = 0.1290 \n",
      "epoch 12/20,step 15/75, loss = 0.0950 \n",
      "epoch 12/20,step 20/75, loss = 0.1142 \n",
      "epoch 12/20,step 25/75, loss = 0.1057 \n",
      "epoch 12/20,step 30/75, loss = 0.1088 \n",
      "epoch 12/20,step 35/75, loss = 0.0495 \n",
      "epoch 12/20,step 40/75, loss = 0.0795 \n",
      "epoch 12/20,step 45/75, loss = 0.0706 \n",
      "epoch 12/20,step 50/75, loss = 0.1206 \n",
      "epoch 12/20,step 55/75, loss = 0.1059 \n",
      "epoch 12/20,step 60/75, loss = 0.0756 \n",
      "epoch 12/20,step 65/75, loss = 0.1520 \n",
      "epoch 12/20,step 70/75, loss = 0.1161 \n",
      "epoch 12/20,step 75/75, loss = 0.1152 \n",
      "F1 score for training set is: 0.9015719806256448\n",
      "Average train loss for epoch: 0.1161015276114146 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8991304347826087\n",
      "epoch 13/20,step 5/75, loss = 0.0709 \n",
      "epoch 13/20,step 10/75, loss = 0.0995 \n",
      "epoch 13/20,step 15/75, loss = 0.0485 \n",
      "epoch 13/20,step 20/75, loss = 0.1417 \n",
      "epoch 13/20,step 25/75, loss = 0.1357 \n",
      "epoch 13/20,step 30/75, loss = 0.0843 \n",
      "epoch 13/20,step 35/75, loss = 0.1383 \n",
      "epoch 13/20,step 40/75, loss = 0.1212 \n",
      "epoch 13/20,step 45/75, loss = 0.1117 \n",
      "epoch 13/20,step 50/75, loss = 0.0689 \n",
      "epoch 13/20,step 55/75, loss = 0.0367 \n",
      "epoch 13/20,step 60/75, loss = 0.0775 \n",
      "epoch 13/20,step 65/75, loss = 0.0458 \n",
      "epoch 13/20,step 70/75, loss = 0.0655 \n",
      "epoch 13/20,step 75/75, loss = 0.0896 \n",
      "F1 score for training set is: 0.9074676008594401\n",
      "Average train loss for epoch: 0.08567789897322654 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8986325157369222\n",
      "epoch 14/20,step 5/75, loss = 0.0883 \n",
      "epoch 14/20,step 10/75, loss = 0.0568 \n",
      "epoch 14/20,step 15/75, loss = 0.0920 \n",
      "epoch 14/20,step 20/75, loss = 0.0435 \n",
      "epoch 14/20,step 25/75, loss = 0.0451 \n",
      "epoch 14/20,step 30/75, loss = 0.0692 \n",
      "epoch 14/20,step 35/75, loss = 0.0159 \n",
      "epoch 14/20,step 40/75, loss = 0.0880 \n",
      "epoch 14/20,step 45/75, loss = 0.0398 \n",
      "epoch 14/20,step 50/75, loss = 0.1153 \n",
      "epoch 14/20,step 55/75, loss = 0.1184 \n",
      "epoch 14/20,step 60/75, loss = 0.0357 \n",
      "epoch 14/20,step 65/75, loss = 0.0329 \n",
      "epoch 14/20,step 70/75, loss = 0.0600 \n",
      "epoch 14/20,step 75/75, loss = 0.0362 \n",
      "F1 score for training set is: 0.91294632072281\n",
      "Average train loss for epoch: 0.059783155992627145 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8818120654855349\n",
      "epoch 15/20,step 5/75, loss = 0.0305 \n",
      "epoch 15/20,step 10/75, loss = 0.0108 \n",
      "epoch 15/20,step 15/75, loss = 0.0645 \n",
      "epoch 15/20,step 20/75, loss = 0.0448 \n",
      "epoch 15/20,step 25/75, loss = 0.0134 \n",
      "epoch 15/20,step 30/75, loss = 0.0244 \n",
      "epoch 15/20,step 35/75, loss = 0.0708 \n",
      "epoch 15/20,step 40/75, loss = 0.0409 \n",
      "epoch 15/20,step 45/75, loss = 0.0359 \n",
      "epoch 15/20,step 50/75, loss = 0.0661 \n",
      "epoch 15/20,step 55/75, loss = 0.0371 \n",
      "epoch 15/20,step 60/75, loss = 0.0535 \n",
      "epoch 15/20,step 65/75, loss = 0.0565 \n",
      "epoch 15/20,step 70/75, loss = 0.0369 \n",
      "epoch 15/20,step 75/75, loss = 0.1202 \n",
      "F1 score for training set is: 0.9178993915541381\n",
      "Average train loss for epoch: 0.04524255724002917 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8753424657534247\n",
      "epoch 16/20,step 5/75, loss = 0.0156 \n",
      "epoch 16/20,step 10/75, loss = 0.0148 \n",
      "epoch 16/20,step 15/75, loss = 0.0296 \n",
      "epoch 16/20,step 20/75, loss = 0.0273 \n",
      "epoch 16/20,step 25/75, loss = 0.0330 \n",
      "epoch 16/20,step 30/75, loss = 0.0163 \n",
      "epoch 16/20,step 35/75, loss = 0.0128 \n",
      "epoch 16/20,step 40/75, loss = 0.0209 \n",
      "epoch 16/20,step 45/75, loss = 0.0334 \n",
      "epoch 16/20,step 50/75, loss = 0.0236 \n",
      "epoch 16/20,step 55/75, loss = 0.0223 \n",
      "epoch 16/20,step 60/75, loss = 0.0154 \n",
      "epoch 16/20,step 65/75, loss = 0.0458 \n",
      "epoch 16/20,step 70/75, loss = 0.0653 \n",
      "epoch 16/20,step 75/75, loss = 0.0104 \n",
      "F1 score for training set is: 0.9224851945179845\n",
      "Average train loss for epoch: 0.029807533491402863 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8691460055096417\n",
      "epoch 17/20,step 5/75, loss = 0.0085 \n",
      "epoch 17/20,step 10/75, loss = 0.0063 \n",
      "epoch 17/20,step 15/75, loss = 0.0351 \n",
      "epoch 17/20,step 20/75, loss = 0.0388 \n",
      "epoch 17/20,step 25/75, loss = 0.0090 \n",
      "epoch 17/20,step 30/75, loss = 0.0522 \n",
      "epoch 17/20,step 35/75, loss = 0.0075 \n",
      "epoch 17/20,step 40/75, loss = 0.0093 \n",
      "epoch 17/20,step 45/75, loss = 0.0295 \n",
      "epoch 17/20,step 50/75, loss = 0.0054 \n",
      "epoch 17/20,step 55/75, loss = 0.0083 \n",
      "epoch 17/20,step 60/75, loss = 0.0343 \n",
      "epoch 17/20,step 65/75, loss = 0.0142 \n",
      "epoch 17/20,step 70/75, loss = 0.0084 \n",
      "epoch 17/20,step 75/75, loss = 0.0334 \n",
      "F1 score for training set is: 0.9266456327967857\n",
      "Average train loss for epoch: 0.021539100219185155 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8650646950092421\n",
      "epoch 18/20,step 5/75, loss = 0.0133 \n",
      "epoch 18/20,step 10/75, loss = 0.0100 \n",
      "epoch 18/20,step 15/75, loss = 0.0149 \n",
      "epoch 18/20,step 20/75, loss = 0.0034 \n",
      "epoch 18/20,step 25/75, loss = 0.0149 \n",
      "epoch 18/20,step 30/75, loss = 0.0331 \n",
      "epoch 18/20,step 35/75, loss = 0.0049 \n",
      "epoch 18/20,step 40/75, loss = 0.0108 \n",
      "epoch 18/20,step 45/75, loss = 0.0333 \n",
      "epoch 18/20,step 50/75, loss = 0.0024 \n",
      "epoch 18/20,step 55/75, loss = 0.0172 \n",
      "epoch 18/20,step 60/75, loss = 0.0120 \n",
      "epoch 18/20,step 65/75, loss = 0.0095 \n",
      "epoch 18/20,step 70/75, loss = 0.0019 \n",
      "epoch 18/20,step 75/75, loss = 0.0243 \n",
      "F1 score for training set is: 0.9304291055849011\n",
      "Average train loss for epoch: 0.013499867718977232 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8714547118023788\n",
      "epoch 19/20,step 5/75, loss = 0.0086 \n",
      "epoch 19/20,step 10/75, loss = 0.0049 \n",
      "epoch 19/20,step 15/75, loss = 0.0034 \n",
      "epoch 19/20,step 20/75, loss = 0.0028 \n",
      "epoch 19/20,step 25/75, loss = 0.0097 \n",
      "epoch 19/20,step 30/75, loss = 0.0152 \n",
      "epoch 19/20,step 35/75, loss = 0.0028 \n",
      "epoch 19/20,step 40/75, loss = 0.0031 \n",
      "epoch 19/20,step 45/75, loss = 0.0028 \n",
      "epoch 19/20,step 50/75, loss = 0.0021 \n",
      "epoch 19/20,step 55/75, loss = 0.0126 \n",
      "epoch 19/20,step 60/75, loss = 0.0230 \n",
      "epoch 19/20,step 65/75, loss = 0.0089 \n",
      "epoch 19/20,step 70/75, loss = 0.0036 \n",
      "epoch 19/20,step 75/75, loss = 0.0060 \n",
      "F1 score for training set is: 0.933867292874705\n",
      "Average train loss for epoch: 0.010671736779622734 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8562748305678898\n",
      "epoch 20/20,step 5/75, loss = 0.0106 \n",
      "epoch 20/20,step 10/75, loss = 0.0070 \n",
      "epoch 20/20,step 15/75, loss = 0.0332 \n",
      "epoch 20/20,step 20/75, loss = 0.2036 \n",
      "epoch 20/20,step 25/75, loss = 0.0159 \n",
      "epoch 20/20,step 30/75, loss = 0.0407 \n",
      "epoch 20/20,step 35/75, loss = 0.0727 \n",
      "epoch 20/20,step 40/75, loss = 0.0100 \n",
      "epoch 20/20,step 45/75, loss = 0.0198 \n",
      "epoch 20/20,step 50/75, loss = 0.0175 \n",
      "epoch 20/20,step 55/75, loss = 0.0082 \n",
      "epoch 20/20,step 60/75, loss = 0.0277 \n",
      "epoch 20/20,step 65/75, loss = 0.0036 \n",
      "epoch 20/20,step 70/75, loss = 0.0038 \n",
      "epoch 20/20,step 75/75, loss = 0.0117 \n",
      "F1 score for training set is: 0.9367469233507916\n",
      "Average train loss for epoch: 0.029380885149973134 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:04<00:00, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for validation set is: 0.8764452505100883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainingLoop(train_loader, modelNN, loss_func, criterion, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
